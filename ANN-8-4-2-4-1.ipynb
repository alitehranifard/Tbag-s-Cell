{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "250c3764-e8b7-45ed-bb88-77cca89e7132",
   "metadata": {},
   "source": [
    "## importing libraries & weights and biass initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1ba2be3f-6f8b-40df-97e0-f6464355222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bcefeac6-9f6c-4158-8e1a-a3f24d83c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting random seed for reproductibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65c0e4ab-c399-4302-954e-4ae56458753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN Structure \n",
    "size_of_input = 8\n",
    "size_of_hiddens = [4,2,4]\n",
    "size_of_output = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3cbd7edb-25f6-488d-897d-7609231bd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize Weights and Biases between random numbers from 0 to 1\n",
    "weights = {\n",
    "    \"W1\": np.random.rand(size_of_input, size_of_hiddens[0]),\n",
    "    \"W2\": np.random.rand(size_of_hiddens[0], size_of_hiddens[1]),\n",
    "    \"W3\": np.random.rand(size_of_hiddens[1], size_of_hiddens[2]),\n",
    "    \"W4\": np.random.rand(size_of_hiddens[2], size_of_output),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"b1\": np.random.rand(1, size_of_hiddens[0]),\n",
    "    \"b2\": np.random.rand(1, size_of_hiddens[1]),\n",
    "    \"b3\": np.random.rand(1, size_of_hiddens[2]),\n",
    "    \"b4\": np.random.rand(1, size_of_output),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "754822e5-e06e-482d-a5d0-acce57e11b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape: (8, 4)\n",
      "\n",
      "\n",
      "W2 shape: (4, 2)\n",
      "\n",
      "\n",
      "W3 shape: (2, 4)\n",
      "\n",
      "\n",
      "W4 shape: (4, 1)\n",
      "\n",
      "\n",
      "b1 shape: (1, 4)\n",
      "\n",
      "\n",
      "b2 shape: (1, 2)\n",
      "\n",
      "\n",
      "b3 shape: (1, 4)\n",
      "\n",
      "\n",
      "b4 shape: (1, 1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print shapes to verify sizes\n",
    "for key, value in weights.items():\n",
    "    print(f\"{key} shape: {value.shape}\")\n",
    "    print('\\n')\n",
    "\n",
    "for key, value in biases.items():\n",
    "    print(f\"{key} shape: {value.shape}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da1b52e-027c-45d2-a232-eed6c4b18754",
   "metadata": {},
   "source": [
    "## Forward-Pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d6f37a47-868b-4468-b74f-e608d05f617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of forward pass (predicted classification probabilities):\n",
      "[[0.99999899]]\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def forward_pass(X, weights, biases):\n",
    "    # 1st layer (8 input to 4 hidden)\n",
    "    Z1 = np.dot(X, weights['W1']) + biases['b1']\n",
    "    # applying first Activation Func\n",
    "    A1 = relu(Z1) \n",
    "\n",
    "    # 2nd layer (4 of first hidden layer to 2 of second hidden layer)\n",
    "    Z2 = np.dot(A1, weights['W2']) + biases['b2']\n",
    "    # applying second Activation Func\n",
    "    A2 = relu(Z2) \n",
    "\n",
    "    # 3rd layer (2 of second hidden layer to 4 of third hidden layer)\n",
    "    Z3 = np.dot(A2, weights['W3']) + biases['b3']\n",
    "    # applying third Activation Func\n",
    "    A3 = relu(Z3)\n",
    "\n",
    "    # 4th Layer (4 of third hidden layer to 1 of output layer\n",
    "    Z4 = np.dot(A3, weights['W4']) + biases['b4']\n",
    "    # appying forth Activation Func\n",
    "    A4= sigmoid(Z4)\n",
    "\n",
    "    # returning A1-A4 for BackPropagation porpuses\n",
    "    return A1, A2, A3, A4\n",
    "\n",
    "## testing the forward pass func\n",
    "np.random.seed(0)\n",
    "TestX = np.random.rand(1, size_of_input)\n",
    "A1, A2, A3, A4 = forward_pass(TestX, weights, biases)\n",
    "\n",
    "print(\"Output of forward pass (predicted classification probabilities):\")\n",
    "print(A4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a001c2-fdd6-46aa-834d-7a8ecb20fcd5",
   "metadata": {},
   "source": [
    "## Backpropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0f40953f-3d29-4520-9e8e-2b259bbc07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Derivative of ReLU\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "## Binary Loss Function (Cross-Entropy)\n",
    "def binary_loss(y_true, y_pred):\n",
    "    M = y_true.shape[0] ## Number of samples ( rows )\n",
    "    loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / M\n",
    "    return loss\n",
    "    \n",
    "## Backpropagation function\n",
    "def backpropagation(X, y, weights, biases, learning_rate=0.01):\n",
    "    M = X.shape[0] # Number of samples ( rows ) \n",
    "    # Forward Pass\n",
    "    A1, A2, A3, A4 = forward_pass(X, weights, biases)\n",
    "\n",
    "    ## Output layer error\n",
    "    dZ4 = A4 - y ## Deriv of the loss func with respect to A4\n",
    "\n",
    "    ## Gradients of the output layer\n",
    "    dW4 = np.dot(A3.T, dZ4) / M ## Gradient of weights for W4\n",
    "    db4 = np.sum(dZ4, axis=0, keepdims=True) / M ## Gradient of bias for b3\n",
    "\n",
    "    ## Backpropagate to hidden layer 3\n",
    "    dA3 = np.dot(dZ4, weights[\"W4\"].T)  ## Gradient of A3 (from dZ4)\n",
    "    dZ3 = dA3 * relu_derivative(A3)  ## Derivative of ReLU for A3\n",
    "    \n",
    "    ## Gradients for Hidden Layer 3\n",
    "    dW3 = np.dot(A2.T, dZ3) / M  ## Gradient of weights for W3\n",
    "    db3 = np.sum(dZ3, axis=0, keepdims=True) / M  ## Gradient of bias for b3\n",
    "    \n",
    "    ## Backpropagate to hidden layer 2\n",
    "    dA2 = np.dot(dZ3, weights[\"W3\"].T)  ## Gradient of A2 (from dZ3)\n",
    "    dZ2 = dA2 * relu_derivative(A2)  ## Derivative of ReLU for A2\n",
    "    \n",
    "    ## Gradients for Hidden Layer 2\n",
    "    dW2 = np.dot(A1.T, dZ2) / M  ## Gradient of weights for W2\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / M  ## Gradient of bias for b2\n",
    "    \n",
    "    ## Backpropagate to hidden layer 1\n",
    "    dA1 = np.dot(dZ2, weights[\"W2\"].T)  ## Gradient of A1 (from dZ2)\n",
    "    dZ1 = dA1 * relu_derivative(A1)  ## Derivative of ReLU for A1\n",
    "    \n",
    "    ## Gradients for Hidden Layer 1\n",
    "    dW1 = np.dot(X.T, dZ1) / M  ## Gradient of weights for W1\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / M  ## Gradient of bias for b1\n",
    "    \n",
    "    ## Update weights and biases using gradient descent\n",
    "    weights[\"W1\"] -= learning_rate * dW1\n",
    "    weights[\"W2\"] -= learning_rate * dW2\n",
    "    weights[\"W3\"] -= learning_rate * dW3\n",
    "    weights[\"W4\"] -= learning_rate * dW4\n",
    "    \n",
    "    biases[\"b1\"] -= learning_rate * db1\n",
    "    biases[\"b2\"] -= learning_rate * db2\n",
    "    biases[\"b3\"] -= learning_rate * db3\n",
    "    biases[\"b4\"] -= learning_rate * db4\n",
    "    \n",
    "    # Return the loss after backpropagation\n",
    "    loss = binary_loss(y, A4)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1631af4-659e-4fa6-9e93-b9eca7570917",
   "metadata": {},
   "source": [
    "## Example input and labels ( random ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "03b3d375-4526-4b61-bd3d-00690b5f97d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 9.79649\n",
      "Epoch 5, Loss: 4.55347\n",
      "Epoch 10, Loss: 1.84276\n",
      "Epoch 15, Loss: 0.75787\n",
      "Epoch 20, Loss: 0.67549\n",
      "Epoch 25, Loss: 0.67243\n",
      "Epoch 30, Loss: 0.67207\n",
      "Epoch 35, Loss: 0.67180\n",
      "Epoch 40, Loss: 0.67153\n",
      "Epoch 45, Loss: 0.67126\n",
      "Epoch 50, Loss: 0.67100\n",
      "Epoch 55, Loss: 0.67073\n",
      "Epoch 60, Loss: 0.67046\n",
      "Epoch 65, Loss: 0.67019\n",
      "Epoch 70, Loss: 0.66993\n",
      "Epoch 75, Loss: 0.66966\n",
      "Epoch 80, Loss: 0.66939\n",
      "Epoch 85, Loss: 0.66912\n",
      "Epoch 90, Loss: 0.66885\n",
      "Epoch 95, Loss: 0.66858\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "X_train = np.random.rand(5, 8)  # 5 samples, 8 features\n",
    "y_train = np.random.randint(0, 2, (5, 1))  # 5 labels (binary)\n",
    "\n",
    "# Initialize random weights and biases \n",
    "weights = {  \n",
    "    \"W1\": np.random.rand(8, 4),\n",
    "    \"W2\": np.random.rand(4, 2),\n",
    "    \"W3\": np.random.rand(2, 4),\n",
    "    \"W4\": np.random.rand(4, 1),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    \"b1\": np.random.rand(1, 4),\n",
    "    \"b2\": np.random.rand(1, 2),\n",
    "    \"b3\": np.random.rand(1, 4),\n",
    "    \"b4\": np.random.rand(1, 1),\n",
    "}\n",
    "\n",
    "# Training loop (25 iterations)\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "for epoch in range(num_epochs):\n",
    "    # Perform Backpropagation and bring back the Loss\n",
    "    loss = backpropagation(X_train, y_train, weights, biases, learning_rate)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
